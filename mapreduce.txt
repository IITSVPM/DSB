Create a folder name analyze manually in Download. Copy all the file (3 java, 2 txt and 1 csv) in this folder. Then follow all the commands.   


hduser@svpmit31-HP-EliteDesk-800-G2-SFF:~$ su - hduser
Password: 
hduser@svpmit31-HP-EliteDesk-800-G2-SFF:~$ pwd
/home/hduser
hduser@svpmit31-HP-EliteDesk-800-G2-SFF:~$ sudo mkdir analyze0

/*using this command you are creating a folder analyze0. In this you’ll copy the content of folder analyze so keep the names as it is in the commands*/

[sudo] password for hduser: 
hduser@svpmit31-HP-EliteDesk-800-G2-SFF:~$ ls
analyzalog9  analyzelog1   analyzelogs1  Desktop    examples.desktop  Music     Templates
analyze0     analyzelog10  analze        Documents  input2000         Pictures  Videos
analyzelog   analyzelogs   derby.log     Downloads  metastore_db      Public    workspace

hduser@svpmit31-HP-EliteDesk-800-G2-SFF:~$ cd analyze0
hduser@svpmit31-HP-EliteDesk-800-G2-SFF:~/analyze0$ ls
hduser@svpmit31-HP-EliteDesk-800-G2-SFF:~/analyze0$ sudo cp /home/hduser/Downloads/analyze/* ~/analyze0
hduser@svpmit31-HP-EliteDesk-800-G2-SFF:~/analyze0$ ls
access_log_short_csv.csv  access_log_short.txt  log_analysis_main.java  log_mapper.java  log_reducer.java  Manifest.txt

hduser@svpmit31-HP-EliteDesk-800-G2-SFF:~/analyze0$ start-dfs.sh
25/04/16 12:49:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Starting namenodes on [localhost]
hduser@localhost's password: 
localhost: namenode running as process 8897. Stop it first.
hduser@localhost's password: 
localhost: datanode running as process 9072. Stop it first.
Starting secondary namenodes [0.0.0.0]
hduser@0.0.0.0's password: 
0.0.0.0: secondarynamenode running as process 9284. Stop it first.
25/04/16 12:49:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
hduser@svpmit31-HP-EliteDesk-800-G2-SFF:~/analyze0$ start-yarn.sh
starting yarn daemons
resourcemanager running as process 9456. Stop it first.
hduser@localhost's password: 
localhost: nodemanager running as process 9782. Stop it first.

hduser@svpmit31-HP-EliteDesk-800-G2-SFF:~/analyze0$ cd ..
hduser@svpmit31-HP-EliteDesk-800-G2-SFF:~$ sudo chmod -R 777 analyze0/
hduser@svpmit31-HP-EliteDesk-800-G2-SFF:~$ sudo chown -R hduser analyze0/
hduser@svpmit31-HP-EliteDesk-800-G2-SFF:~$ sudo chmod +r *.*

hduser@svpmit31-HP-EliteDesk-800-G2-SFF:~$ export CLASSPATH="$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.9.0.jar:$HADOOP_HOME/share/hadoop/mapreduce/hadoopmapreduce-client-common-2.9.0.jar:$HADOOP_HOME/share/hadoop/common/hadoop-common-2.9.0.jar:/home/hduser/analyze0/*:$HADOOP_HOME/lib/*"

hduser@svpmit31-HP-EliteDesk-800-G2-SFF:~$ cd analyze0
hduser@svpmit31-HP-EliteDesk-800-G2-SFF:~/analyze0$ ls
access_log_short_csv.csv  access_log_short.txt  log_analysis_main.java  log_mapper.java  log_reducer.java  Manifest.txt
hduser@svpmit31-HP-EliteDesk-800-G2-SFF:~/analyze0$ javac -d . log_mapper.java log_reducer.java log_analysis_main.java
hduser@svpmit31-HP-EliteDesk-800-G2-SFF:~/analyze0$ ls
access_log_short_csv.csv  access_log_short.txt  loganalysis  log_analysis_main.java  log_mapper.java  log_reducer.java  Manifest.txt
hduser@svpmit31-HP-EliteDesk-800-G2-SFF:~/analyze0$ jar -cfm analyze0.jar Manifest.txt loganalysis/*.class
hduser@svpmit31-HP-EliteDesk-800-G2-SFF:~/analyze0$ start-dfs.sh
25/04/16 13:06:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Starting namenodes on [localhost]
hduser@localhost's password: 
localhost: namenode running as process 8897. Stop it first.
hduser@localhost's password: 
localhost: datanode running as process 9072. Stop it first.
Starting secondary namenodes [0.0.0.0]
hduser@0.0.0.0's password: 
0.0.0.0: secondarynamenode running as process 9284. Stop it first.
25/04/16 13:06:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
hduser@svpmit31-HP-EliteDesk-800-G2-SFF:~/analyze0$ start-yarn.sh
starting yarn daemons
resourcemanager running as process 9456. Stop it first.
hduser@localhost's password: 
localhost: nodemanager running as process 9782. Stop it first.
hduser@svpmit31-HP-EliteDesk-800-G2-SFF:~/analyze0$ jps
9456 ResourceManager
9072 DataNode
20928 Jps
8897 NameNode
9284 SecondaryNameNode
9782 NodeManager

hduser@svpmit31-HP-EliteDesk-800-G2-SFF:~/analyze0$ sudo mkdir ~/input4000
hduser@svpmit31-HP-EliteDesk-800-G2-SFF:~/analyze0$ sudo cp access_log_short_csv.csv ~/input4000/

hduser@svpmit31-HP-EliteDesk-800-G2-SFF:~/analyze0$ ls
access_log_short_csv.csv  analyze0.jar  log_analysis_main.java  log_reducer.java
access_log_short.txt      loganalysis   log_mapper.java         Manifest.txt
hduser@svpmit31-HP-EliteDesk-800-G2-SFF:~/analyze0$ HADOOP_HOME/bin/hdfs dfs -put ~/input4000 /
-su: HADOOP_HOME/bin/hdfs: No such file or directory

hduser@svpmit31-HP-EliteDesk-800-G2-SFF:~/analyze0$ hdfs dfs -put ~/input4000 /
25/04/16 13:19:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
hduser@svpmit31-HP-EliteDesk-800-G2-SFF:~/analyze0$ hadoop jar analyze0.jar /input4000 /output4000
25/04/16 13:20:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/04/16 13:20:36 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
25/04/16 13:20:37 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
25/04/16 13:20:37 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
25/04/16 13:20:37 INFO mapred.FileInputFormat: Total input files to process : 1
25/04/16 13:20:37 INFO mapreduce.JobSubmitter: number of splits:2
25/04/16 13:20:37 INFO Configuration.deprecation: yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled
25/04/16 13:20:37 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1744779020692_0001
25/04/16 13:20:38 INFO impl.YarnClientImpl: Submitted application application_1744779020692_0001
25/04/16 13:20:38 INFO mapreduce.Job: The url to track the job: http://svpmit31-HP-EliteDesk-800-G2-SFF:8088/proxy/application_1744779020692_0001/
25/04/16 13:20:38 INFO mapreduce.Job: Running job: job_1744779020692_0001
25/04/16 13:20:43 INFO mapreduce.Job: Job job_1744779020692_0001 running in uber mode : false
25/04/16 13:20:43 INFO mapreduce.Job:  map 0% reduce 0%
25/04/16 13:20:47 INFO mapreduce.Job:  map 100% reduce 0%
25/04/16 13:20:52 INFO mapreduce.Job:  map 100% reduce 100%
25/04/16 13:20:52 INFO mapreduce.Job: Job job_1744779020692_0001 completed successfully
25/04/16 13:20:52 INFO mapreduce.Job: Counters: 49
	File System Counters
		FILE: Number of bytes read=26771
		FILE: Number of bytes written=659349
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=162863
		HDFS: Number of bytes written=3838
		HDFS: Number of read operations=9
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=2
		Launched reduce tasks=1
		Data-local map tasks=2
		Total time spent by all maps in occupied slots (ms)=4486
		Total time spent by all reduces in occupied slots (ms)=1596
		Total time spent by all map tasks (ms)=4486
		Total time spent by all reduce tasks (ms)=1596
		Total vcore-milliseconds taken by all map tasks=4486
		Total vcore-milliseconds taken by all reduce tasks=1596
		Total megabyte-milliseconds taken by all map tasks=4593664
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=162647
	File Output Format Counters 
		Bytes Written=3838
hduser@svpmit31-HP-EliteDesk-800-G2-SFF:~/analyze0$ hdfs dfs -cat /output4000/part-00000
25/04/16 13:22:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
10.1.1.236 	7
10.1.181.142 	14
10.1.232.31 	5
10.10.55.142 	14
10.102.101.66 	1
10.103.184.104 	1
10.103.190.81 	53
10.103.63.29 	1
10.104.73.51 	1
10.105.160.183 	1
10.108.91.151 	1
10.109.21.76 	1
10.11.131.40 	1
10.111.71.20 	8
10.112.227.184 	6
10.114.74.30 	1
10.115.118.78 	1
10.117.224.230 	1
10.117.76.22 	12
10.118.19.97 	1
10.118.250.30 	7
10.119.117.132 	23
10.119.33.245 	1
10.119.74.120 	1
10.12.113.198 	2
10.12.219.30 	1
10.120.165.113 	1
10.120.207.127 	4
10.123.124.47 	1
10.123.35.235 	1
10.124.148.99 	1
10.124.155.234 	1
10.126.161.13 	7
10.127.162.239 	1
10.128.11.75 	10
10.13.42.232 	1
10.130.195.163 	8
10.130.70.80 	1
10.131.163.73 	1
10.131.209.116 	5
10.132.19.125 	2
10.133.222.184 	12
10.134.110.196 	13
10.134.242.87 	1
10.136.84.60 	5
10.14.2.86 	8
10.14.4.151 	2
10.140.139.116 	1
10.140.141.1 	9
10.140.67.116 	1
10.141.221.57 	5
10.142.203.173 	7
10.143.126.177 	32
10.144.147.8 	1
10.15.208.56 	1
10.15.23.44 	13
10.150.212.239 	14
10.150.227.16 	1
10.150.24.40 	13
10.152.195.138 	8
10.153.23.63 	2
10.153.239.5 	25
10.155.95.124 	9
10.156.152.9 	1
10.157.176.158 	1
10.186.56.183 	1
10.187.129.140 	6
10.187.177.220 	1
10.187.212.83 	1
10.187.28.68 	1
10.19.226.186 	2
10.190.174.142 	10
10.190.41.42 	5
10.191.172.11 	1
10.193.116.91 	1
10.194.174.4 	7
10.198.138.192 	1
10.199.103.248 	2
10.199.189.15 	1
10.2.202.135 	1
10.200.184.212 	1
10.200.237.222 	1
10.200.9.128 	2
10.203.194.139 	10
10.205.72.238 	2
10.206.108.96 	2
10.206.175.236 	1
10.206.73.206 	7
10.207.190.45 	17
10.208.38.46 	1
10.208.49.216 	4
10.209.18.39 	9
10.209.54.187 	3
10.211.47.159 	10
10.212.122.173 	1
10.213.181.38 	7
10.214.35.48 	1
10.215.222.114 	1
10.216.113.172 	48
10.216.134.214 	1
10.216.227.195 	16
10.217.151.145 	10
10.32.138.48 	11
10.32.247.175 	4
10.32.55.216 	12
10.33.181.9 	8
10.34.233.107 	1
10.36.200.176 	1
10.39.45.70 	2
10.39.94.109 	4
10.4.59.153 	1
10.4.79.47 	15
10.41.170.233 	9
10.41.40.17 	1
10.42.208.60 	1
10.43.81.13 	1
10.46.190.95 	10
10.48.81.158 	5
10.5.132.217 	1
10.5.148.29 	1
10.50.226.223 	9
10.50.41.216 	3
10.52.161.126 	1
10.87.88.214 	1
10.88.204.177 	1
10.89.178.62 	1
10.89.244.42 	1
10.94.196.42 	1
10.95.136.211 	4
10.95.232.88 	1
10.98.156.141 	1
10.99.228.224 	1
hduser@svpmit31-HP-EliteDesk-800-G2-SFF:~/analyze0$ 

1. su - hduser
Purpose: Switches to the Hadoop user account (hduser) and loads that user's environment.

Why: Hadoop is usually run under a separate user (hduser) for security and organization.

2. sudo mkdir analyze0
Purpose: Creates a directory named analyze0 with superuser (root) privileges.

Why: This folder will store the MapReduce Java code and related files.

3. sudo cp /home/hduser/Downloads/analyze/* ~/analyze0
Purpose: Copies all files from Downloads/analyze into the analyze0 directory.

Why: You're setting up the working directory with source code files for compilation.

4. start-dfs.sh
Purpose: Starts Hadoop HDFS daemons (NameNode and DataNode).

Why: Required for storing and retrieving data in Hadoop Distributed File System (HDFS).

5. start-yarn.sh
Purpose: Starts the YARN daemons (ResourceManager and NodeManager).

Why: Required to manage cluster resources and schedule MapReduce jobs.

6. sudo chmod -R 777 analyze0/
Purpose: Gives full read, write, and execute permissions to everyone for the analyze0 directory.

Why: Ensures you won’t run into permission errors while working with files in that folder.

7. sudo chown -R hduser analyze0/
Purpose: Changes the ownership of the analyze0 directory to the hduser.

Why: Ensures the hduser can modify the files without needing sudo.

8. sudo chmod +r *.*
Purpose: Grants read permission to all files with extensions (e.g., .java, .txt).

Why: Ensures files are readable before compilation or execution.

9. Set Hadoop CLASSPATH for compilation
export CLASSPATH="$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.9.0.jar:$HADOOP_HOME/share/hadoop/mapreduce/hadoopmapreduce-client-common-2.9.0.jar:$HADOOP_HOME/share/hadoop/common/hadoop-common-2.9.0.jar:/home/hduser/analyze0/*:$HADOOP_HOME/lib/*"
Purpose: Sets Java classpath to include necessary Hadoop and user-defined JARs.

Why: Required for compiling MapReduce programs using Hadoop libraries.

10. javac -d . log_mapper.java log_reducer.java log_analysis_main.java
Purpose: Compiles the three Java files into class files.

Why: Converts Java source code into bytecode that Hadoop can execute.

11. jar -cfm analyze0.jar Manifest.txt loganalysis/*.class
Purpose: Creates a JAR file named analyze0.jar using the manifest and compiled .class files.

Why: Hadoop requires your job to be packaged as a .jar to run it on the cluster.

12. sudo mkdir ~/input4000
Purpose: Creates a new directory named input4000 in the current user's home directory.

Why: This will contain the input data file (like log files) for your MapReduce job.

13. sudo cp access_log_short_csv.csv ~/input4000/
Purpose: Copies your CSV input file into the input4000 directory.

Why: Prepares the input data for uploading into HDFS.

14. hdfs dfs -put ~/input4000 /
Purpose: Uploads input4000 directory from local FS to the root of HDFS.

Why: Hadoop jobs can only read files from HDFS, not local file system.

15. hdfs dfs -cat /output4000/part-00000
Purpose: Displays the output of your MapReduce job stored in output4000 directory on HDFS.

Why: Allows you to verify the job's result after execution.
